import numpy as np

# activation functions
def relu(x):
    return np.maximum(0, x)

def sigmoid(x):
    return 1 / (1 + np.exp(-x))


class FeedForward2HL:
    def __init__(self):   # <-- FIXED constructor
        # GIVEN WEIGHTS
        self.W1 = np.array([0.5, -0.2, 0.3])     # Hidden Layer 1 weights
        self.W2 = np.array([0.4, 0.1, -0.5])     # Hidden Layer 2 weights

        self.Wout = 1  # Output weight

    def forward(self, x):
        # Hidden Layer 1 → ReLU
        z1 = np.sum(x * self.W1)
        h1 = relu(z1)

        # Hidden Layer 2 → Sigmoid
        z2 = h1 * np.sum(self.W2)  # simplified
        h2 = sigmoid(z2)

        # Output → Sigmoid
        z3 = h2 * self.Wout
        output = sigmoid(z3)

        return output


# --------- RUN EXAMPLE ---------
x = np.array([1, 0, 1])   # GIVEN INPUT VECTOR

model = FeedForward2HL()
result = model.forward(x)

print("Final Output:", result)
